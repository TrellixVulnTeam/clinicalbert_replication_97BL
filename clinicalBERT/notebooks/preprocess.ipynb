{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocess.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "%cd '/content/gdrive/My Drive/Colab Notebooks/DL4H Project' "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woB0NqovPyXa",
        "outputId": "40ee1b4a-b405-49b8-fea0-f22e689e64e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/Colab Notebooks/DL4H Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "NxAeKCzeRZNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_adm = pd.read_csv(r'./Data/ADMISSIONS.csv')\n",
        "df_adm.ADMITTIME = pd.to_datetime(df_adm.ADMITTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
        "df_adm.DISCHTIME = pd.to_datetime(df_adm.DISCHTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
        "df_adm.DEATHTIME = pd.to_datetime(df_adm.DEATHTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
        "\n",
        "df_adm = df_adm.sort_values(['SUBJECT_ID','ADMITTIME'])\n",
        "df_adm = df_adm.reset_index(drop = True)\n",
        "df_adm['NEXT_ADMITTIME'] = df_adm.groupby('SUBJECT_ID').ADMITTIME.shift(-1)\n",
        "df_adm['NEXT_ADMISSION_TYPE'] = df_adm.groupby('SUBJECT_ID').ADMISSION_TYPE.shift(-1)\n",
        "\n",
        "rows = df_adm.NEXT_ADMISSION_TYPE == 'ELECTIVE'\n",
        "df_adm.loc[rows,'NEXT_ADMITTIME'] = pd.NaT\n",
        "df_adm.loc[rows,'NEXT_ADMISSION_TYPE'] = np.NaN\n",
        "\n",
        "df_adm = df_adm.sort_values(['SUBJECT_ID','ADMITTIME'])\n",
        "\n",
        "#When we filter out the \"ELECTIVE\", we need to correct the next admit time for these admissions since there might be 'emergency' next admit after \"ELECTIVE\"\n",
        "df_adm[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']] = df_adm.groupby(['SUBJECT_ID'])[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']].fillna(method = 'bfill')\n",
        "df_adm['DAYS_NEXT_ADMIT']=  (df_adm.NEXT_ADMITTIME - df_adm.DISCHTIME).dt.total_seconds()/(24*60*60)\n",
        "df_adm['OUTPUT_LABEL'] = (df_adm.DAYS_NEXT_ADMIT < 30).astype('int')\n",
        "### filter out newborn and death\n",
        "df_adm = df_adm[df_adm['ADMISSION_TYPE']!='NEWBORN']\n",
        "df_adm = df_adm[df_adm.DEATHTIME.isnull()]\n",
        "df_adm['DURATION'] = (df_adm['DISCHTIME']-df_adm['ADMITTIME']).dt.total_seconds()/(24*60*60)"
      ],
      "metadata": {
        "id": "nuDvtf_XRzQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_notes = pd.read_csv('./Data/NOTEEVENTS.csv')\n",
        "df_notes = df_notes.sort_values(by=['SUBJECT_ID','HADM_ID','CHARTDATE'])\n",
        "df_adm_notes = pd.merge(df_adm[['SUBJECT_ID','HADM_ID','ADMITTIME','DISCHTIME','DAYS_NEXT_ADMIT','NEXT_ADMITTIME','ADMISSION_TYPE','DEATHTIME','OUTPUT_LABEL','DURATION']],\n",
        "                        df_notes[['SUBJECT_ID','HADM_ID','CHARTDATE','TEXT','CATEGORY']], \n",
        "                        on = ['SUBJECT_ID','HADM_ID'],\n",
        "                        how = 'left')\n",
        "\n",
        "df_adm_notes.ADMITTIME_C = df_adm_notes.ADMITTIME.apply(lambda x: str(x).split(' ')[0])\n",
        "df_adm_notes['ADMITTIME_C'] = pd.to_datetime(df_adm_notes.ADMITTIME_C, format = '%Y-%m-%d', errors = 'coerce')\n",
        "df_adm_notes['CHARTDATE'] = pd.to_datetime(df_adm_notes.CHARTDATE, format = '%Y-%m-%d', errors = 'coerce')\n",
        "\n",
        "### If Discharge Summary \n",
        "df_discharge = df_adm_notes[df_adm_notes['CATEGORY'] == 'Discharge summary']\n",
        "# multiple discharge summary for one admission -> after examination -> replicated summary -> replace with the last one \n",
        "df_discharge = (df_discharge.groupby(['SUBJECT_ID','HADM_ID']).nth(-1)).reset_index()\n",
        "df_discharge=df_discharge[df_discharge['TEXT'].notnull()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCe_2lpESiVP",
        "outputId": "1baa2c4f-87b9-4b40-fbe5-63224c721496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### If Less than n days on admission notes (Early notes)\n",
        "def less_n_days_data (df_adm_notes, n):\n",
        "    \n",
        "    df_less_n = df_adm_notes[((df_adm_notes['CHARTDATE']-df_adm_notes['ADMITTIME_C']).dt.total_seconds()/(24*60*60))<n]\n",
        "    df_less_n=df_less_n[df_less_n['TEXT'].notnull()]\n",
        "    #concatenate first\n",
        "    df_concat = pd.DataFrame(df_less_n.groupby('HADM_ID')['TEXT'].apply(lambda x: \"%s\" % ' '.join(x))).reset_index()\n",
        "    df_concat['OUTPUT_LABEL'] = df_concat['HADM_ID'].apply(lambda x: df_less_n[df_less_n['HADM_ID']==x].OUTPUT_LABEL.values[0])\n",
        "\n",
        "    return df_concat\n",
        "\n",
        "df_less_2 = less_n_days_data(df_adm_notes, 2)\n",
        "df_less_3 = less_n_days_data(df_adm_notes, 3)"
      ],
      "metadata": {
        "id": "ZaO_vbCBTMxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def preprocess1(x):\n",
        "    y=re.sub('\\\\[(.*?)\\\\]','',x) #remove de-identified brackets\n",
        "    y=re.sub('[0-9]+\\.','',y) #remove 1.2. since the segmenter segments based on this\n",
        "    y=re.sub('dr\\.','doctor',y)\n",
        "    y=re.sub('m\\.d\\.','md',y)\n",
        "    y=re.sub('admission date:','',y)\n",
        "    y=re.sub('discharge date:','',y)\n",
        "    y=re.sub('--|__|==','',y)\n",
        "    return y\n",
        "\n",
        "def preprocessing(df_less_n): \n",
        "    df_less_n['TEXT']=df_less_n['TEXT'].fillna(' ')\n",
        "    df_less_n['TEXT']=df_less_n['TEXT'].str.replace('\\n',' ')\n",
        "    df_less_n['TEXT']=df_less_n['TEXT'].str.replace('\\r',' ')\n",
        "    df_less_n['TEXT']=df_less_n['TEXT'].apply(str.strip)\n",
        "    df_less_n['TEXT']=df_less_n['TEXT'].str.lower()\n",
        "\n",
        "    df_less_n['TEXT']=df_less_n['TEXT'].apply(lambda x: preprocess1(x))\n",
        "\n",
        "    #to get 318 words chunks for readmission tasks\n",
        "    from tqdm import tqdm\n",
        "    df_len = len(df_less_n)\n",
        "    want=pd.DataFrame({'ID':[],'TEXT':[],'Label':[]})\n",
        "    for i in tqdm(range(df_len)):\n",
        "        x=df_less_n.TEXT.iloc[i].split()\n",
        "        n=int(len(x)/318)\n",
        "        for j in range(n):\n",
        "            want=want.append({'TEXT':' '.join(x[j*318:(j+1)*318]),'Label':df_less_n.OUTPUT_LABEL.iloc[i],'ID':df_less_n.HADM_ID.iloc[i]},ignore_index=True)\n",
        "        if len(x)%318>10:\n",
        "            want=want.append({'TEXT':' '.join(x[-(len(x)%318):]),'Label':df_less_n.OUTPUT_LABEL.iloc[i],'ID':df_less_n.HADM_ID.iloc[i]},ignore_index=True)\n",
        "    \n",
        "    return want\n",
        "\n",
        "df_discharge = preprocessing(df_discharge)\n",
        "df_less_2 = preprocessing(df_less_2)\n",
        "df_less_3 = preprocessing(df_less_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qye350HUTVID",
        "outputId": "b61f4be4-2590-4adb-de3c-1dd4f17ad899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 43880/43880 [30:53<00:00, 23.67it/s]\n",
            "100%|██████████| 44112/44112 [51:45<00:00, 14.20it/s]\n",
            "100%|██████████| 44551/44551 [1:36:11<00:00,  7.72it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_discharge.to_csv('readmission_task.csv')\n",
        "df_less_2.to_csv('less_2_days_notes.csv')\n",
        "df_less_3.to_csv('less_3_days_notes.csv')"
      ],
      "metadata": {
        "id": "B9hrXsjVaWUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "### An example to get the train/test/split with random state:\n",
        "### note that we divide on patient admission level and share among experiments, instead of notes level. \n",
        "### This way, since our methods run on the same set of admissions, we can see the\n",
        "### progression of readmission scores. \n",
        "\n",
        "readmit_ID = df_adm[df_adm.OUTPUT_LABEL == 1].HADM_ID\n",
        "not_readmit_ID = df_adm[df_adm.OUTPUT_LABEL == 0].HADM_ID\n",
        "#subsampling to get the balanced pos/neg numbers of patients for each dataset\n",
        "not_readmit_ID_use = not_readmit_ID.sample(n=len(readmit_ID), random_state=1)\n",
        "id_val_test_t=readmit_ID.sample(frac=0.2,random_state=1)\n",
        "id_val_test_f=not_readmit_ID_use.sample(frac=0.2,random_state=1)\n",
        "\n",
        "id_train_t = readmit_ID.drop(id_val_test_t.index)\n",
        "id_train_f = not_readmit_ID_use.drop(id_val_test_f.index)\n",
        "\n",
        "id_val_t=id_val_test_t.sample(frac=0.5,random_state=1)\n",
        "id_test_t=id_val_test_t.drop(id_val_t.index)\n",
        "\n",
        "id_val_f=id_val_test_f.sample(frac=0.5,random_state=1)\n",
        "id_test_f=id_val_test_f.drop(id_val_f.index)\n",
        "\n",
        "# test if there is overlap between train and test, should return \"array([], dtype=int64)\"\n",
        "(pd.Index(id_test_t).intersection(pd.Index(id_train_t))).values\n",
        "\n",
        "id_test = pd.concat([id_test_t, id_test_f])\n",
        "test_id_label = pd.DataFrame(data = list(zip(id_test, [1]*len(id_test_t)+[0]*len(id_test_f))), columns = ['id','label'])\n",
        "\n",
        "id_val = pd.concat([id_val_t, id_val_f])\n",
        "val_id_label = pd.DataFrame(data = list(zip(id_val, [1]*len(id_val_t)+[0]*len(id_val_f))), columns = ['id','label'])\n",
        "\n",
        "id_train = pd.concat([id_train_t, id_train_f])\n",
        "train_id_label = pd.DataFrame(data = list(zip(id_train, [1]*len(id_train_t)+[0]*len(id_train_f))), columns = ['id','label'])\n",
        "\n",
        "#get discharge train/val/test\n",
        "\n",
        "discharge_train = df_discharge[df_discharge.ID.isin(train_id_label.id)]\n",
        "discharge_val = df_discharge[df_discharge.ID.isin(val_id_label.id)]\n",
        "discharge_test = df_discharge[df_discharge.ID.isin(test_id_label.id)]\n",
        "\n",
        "# subsampling for training....since we obtain training on patient admission level so now we have same number of pos/neg readmission\n",
        "# but each admission is associated with different length of notes and we train on each chunks of notes, not on the admission, we need\n",
        "# to balance the pos/neg chunks on training set. (val and test set are fine) Usually, positive admissions have longer notes, so we need \n",
        "# find some negative chunks of notes from not_readmit_ID that we haven't used yet\n",
        "\n",
        "df = pd.concat([not_readmit_ID_use, not_readmit_ID])\n",
        "df = df.drop_duplicates(keep=False)\n",
        "#check to see if there are overlaps\n",
        "(pd.Index(df).intersection(pd.Index(not_readmit_ID_use))).values\n",
        "\n",
        "# for this set of split with random_state=1, we find we need 400 more negative training samples\n",
        "not_readmit_ID_more = df.sample(n=400, random_state=1)\n",
        "discharge_train_snippets = pd.concat([df_discharge[df_discharge.ID.isin(not_readmit_ID_more)], discharge_train])\n",
        "\n",
        "#shuffle\n",
        "discharge_train_snippets = discharge_train_snippets.sample(frac=1, random_state=1).reset_index(drop=True)\n",
        "\n",
        "#check if balanced\n",
        "discharge_train_snippets.Label.value_counts()\n",
        "\n"
      ],
      "metadata": {
        "id": "6K5NCIKXsO3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60730582-5f13-4c3a-8ab8-b9dd6185eb38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0    13134\n",
              "0.0    13111\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "discharge_train_snippets.to_csv('./discharge/train.csv')\n",
        "discharge_val.to_csv('./discharge/val.csv')\n",
        "discharge_test.to_csv('./discharge/test.csv')\n",
        "\n",
        "### for Early notes experiment: we only need to find training set for 3 days, then we can test \n",
        "### both 3 days and 2 days. Since we split the data on patient level and experiments share admissions\n",
        "### in order to see the progression, the 2 days training dataset is a subset of 3 days training set.\n",
        "### So we only train 3 days and we can test/val on both 2 & 3days or any time smaller than 3 days. This means\n",
        "### if we train on a dataset with all the notes in n days, we can predict readmissions smaller than n days. \n",
        "\n",
        "#for 3 days note, similar to discharge\n",
        "\n",
        "early_train = df_less_3[df_less_3.ID.isin(train_id_label.id)]\n",
        "not_readmit_ID_more = df.sample(n=500, random_state=1)\n",
        "early_train_snippets = pd.concat([df_less_3[df_less_3.ID.isin(not_readmit_ID_more)], early_train])\n",
        "#shuffle\n",
        "early_train_snippets = early_train_snippets.sample(frac=1, random_state=1).reset_index(drop=True)\n",
        "early_train_snippets.to_csv('./3days/train.csv')\n",
        "\n",
        "early_val = df_less_3[df_less_3.ID.isin(val_id_label.id)]\n",
        "early_val.to_csv('./3days/val.csv')\n",
        "\n",
        "# we want to test on admissions that are not discharged already. So for less than 3 days of notes experiment,\n",
        "# we filter out admissions discharged within 3 days\n",
        "actionable_ID_3days = df_adm[df_adm['DURATION'] >= 3].HADM_ID\n",
        "test_actionable_id_label = test_id_label[test_id_label.id.isin(actionable_ID_3days)]\n",
        "early_test = df_less_3[df_less_3.ID.isin(test_actionable_id_label.id)]\n",
        "\n",
        "early_test.to_csv('./3days/test.csv')\n",
        "\n",
        "#for 2 days notes, we only obtain test set. Since the model parameters are tuned on the val set of 3 days\n",
        "\n",
        "actionable_ID_2days = df_adm[df_adm['DURATION'] >= 2].HADM_ID\n",
        "\n",
        "test_actionable_id_label_2days = test_id_label[test_id_label.id.isin(actionable_ID_2days)]\n",
        "\n",
        "early_test_2days = df_less_2[df_less_2.ID.isin(test_actionable_id_label_2days.id)]\n",
        "\n",
        "early_test_2days.to_csv('./2days/test.csv')"
      ],
      "metadata": {
        "id": "MHqLNjzgT4AD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}