{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "create_pretraining_data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4ZxT-cvRmL1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "%cd '/content/gdrive/My Drive/Colab Notebooks/DL4H Project' "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import string"
      ],
      "metadata": {
        "id": "pEA4o8GjRm4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_notes_fold = pd.read_csv('./data/good_datasets/fold1/discharge/train.csv')"
      ],
      "metadata": {
        "id": "EWpjPgikRqVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Preprocessing\n",
        "def preprocess1(x):\n",
        "    y=re.sub('\\\\[(.*?)\\\\]','',x) #remove de-identified brackets\n",
        "    y=re.sub('[0-9]+\\.','',y) #remove 1.2. since the segmenter segments based on this\n",
        "    y=re.sub('dr\\.','doctor',y)\n",
        "    y=re.sub('m\\.d\\.','md',y)\n",
        "    y=re.sub('admission date:','',y)\n",
        "    y=re.sub('discharge date:','',y)\n",
        "    y=re.sub('--|__|==','',y)\n",
        "    \n",
        "    # remove, digits, spaces\n",
        "    y = y.translate(str.maketrans(\"\", \"\", string.digits))\n",
        "    y = \" \".join(y.split())\n",
        "    return y\n",
        "\n",
        "def preprocessing(df_notes): \n",
        "    df_notes['TEXT']=df_notes['TEXT'].fillna(' ')\n",
        "    df_notes['TEXT']=df_notes['TEXT'].str.replace('\\n',' ')\n",
        "    df_notes['TEXT']=df_notes['TEXT'].str.replace('\\r',' ')\n",
        "    df_notes['TEXT']=df_notes['TEXT'].apply(str.strip)\n",
        "    df_notes['TEXT']=df_notes['TEXT'].str.lower()\n",
        "\n",
        "    df_notes['TEXT']=df_notes['TEXT'].apply(lambda x: preprocess1(x))\n",
        "    \n",
        "    return df_notes\n",
        "\n",
        "df_notes_fold = preprocessing(df_notes_fold)"
      ],
      "metadata": {
        "id": "l-CidVGYRs7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Notes to Sentences\n",
        "from spacy.lang.en import English\n",
        "nlp = English()  # just the language with no model\n",
        "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
        "\n",
        "# nlp praser may not work when there is only one token. In these cases, we just remove them as note that has length 1 usually is some random stuff\n",
        "\n",
        "def toSentence(x):\n",
        "    doc = nlp(x)\n",
        "    text=[]\n",
        "    try:\n",
        "        for sent in doc.sents:\n",
        "            st=str(sent).strip() \n",
        "            if len(st)<20:\n",
        "                #a lot of abbreviation is segmented as one line. But these are all describing the previous things\n",
        "                #so I attached it to the sentence before\n",
        "                if len(text)!=0:\n",
        "                    text[-1]=' '.join((text[-1],st))\n",
        "                else:\n",
        "                    text=[st]\n",
        "            else:\n",
        "                text.append((st))\n",
        "    except:\n",
        "        print(doc)\n",
        "    return text\n",
        "\n",
        "pretrain_sent=df_notes_fold['TEXT'].apply(lambda x: toSentence(x))"
      ],
      "metadata": {
        "id": "8xkcVMqPRvIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Create Pretraining File\n",
        "file=open('PRETRAIN_DATA_PATH/clinical_sentences_pretrain_fold11.txt','w')\n",
        "pretrain_sent = pretrain_sent.values\n",
        "for i in tqdm(range(len(pretrain_sent))):\n",
        "    if len(pretrain_sent[i]) > 0:\n",
        "        # remove the one token note\n",
        "        note = pretrain_sent[i]\n",
        "        for sent in note:\n",
        "            file.write(sent+'\\n')\n",
        "        file.write('\\n')"
      ],
      "metadata": {
        "id": "YpAhmM7URtAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # STEP 7: Generate Pretraining Tensorflow TF_Records\n",
        "\n",
        "# Generate datasets for 128 max seq\n",
        " %tensorflow_version 1.x\n",
        "!python create_pretraining_data.py \\\n",
        "  --input_file=PRETRAIN_DATA_PATH/clinical_sentences_pretrain_fold11.txt \\\n",
        "  --output_file=PRETRAIN_DATA_PATH/tf_examples_128_fold11.tfrecord \\\n",
        "  --vocab_file=INITIAL_MODEL_PATH/vocab.txt \\\n",
        "  --do_lower_case=True \\\n",
        "  --max_seq_length=128 \\\n",
        "  --max_predictions_per_seq=20 \\\n",
        "  --masked_lm_prob=0.15 \\\n",
        "  --random_seed=12345 \\\n",
        "  --dupe_factor=3"
      ],
      "metadata": {
        "id": "857S376UR5aM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate datasets for 512 max seq\n",
        "!python create_pretraining_data.py \\\n",
        "  --input_file=PRETRAIN_DATA_PATH/clinical_sentences_pretrain_fold11.txt \\\n",
        "  --output_file=PRETRAIN_DATA_PATH/tf_examples_512_fold11.tfrecord \\\n",
        "  --vocab_file=INITIAL_MODEL_PATH/vocab.txt \\\n",
        "  --do_lower_case=True \\\n",
        "  --max_seq_length=512 \\\n",
        "  --max_predictions_per_seq=76 \\\n",
        "  --masked_lm_prob=0.15 \\\n",
        "  --random_seed=12345 \\\n",
        "  --dupe_factor=3\n"
      ],
      "metadata": {
        "id": "Z-yzwJqcSBdI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}